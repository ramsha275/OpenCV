{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import datetime\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2, math\n",
    "import numpy as np\n",
    "from imutils import face_utils, rotate_bound\n",
    "\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "model = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(model) # link to model: http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "\n",
    "video_capture = cv2.VideoCapture(\"http://192.168.0.102:8080/video\")\n",
    "cv2.imshow('Video', np.empty((5,5),dtype=float))\n",
    "\n",
    "#points are tuples in the form (x,y)\n",
    "# returns angle between points in degrees\n",
    "def calculate_inclination(point1, point2):\n",
    "    x1,x2,y1,y2 = point1[0], point2[0], point1[1], point2[1]\n",
    "    incl = -180/math.pi*math.atan((float(y2-y1))/(x2-x1))\n",
    "    return incl\n",
    "\n",
    "def calculate_boundbox(list_coordinates):\n",
    "    x = min(list_coordinates[:,0])\n",
    "    y = min(list_coordinates[:,1])\n",
    "    w = max(list_coordinates[:,0]) - x\n",
    "    h = max(list_coordinates[:,1]) - y\n",
    "    return (x,y,w,h)\n",
    "\n",
    "def get_face_boundbox(points, face_part):\n",
    "    if face_part == 1:\n",
    "        (x,y,w,h) = calculate_boundbox(points[17:22]) #left eyebrow\n",
    "    elif face_part == 2:\n",
    "        (x,y,w,h) = calculate_boundbox(points[22:27]) #right eyebrow\n",
    "    elif face_part == 3:\n",
    "        (x,y,w,h) = calculate_boundbox(points[36:42]) #left eye\n",
    "    elif face_part == 4:\n",
    "        (x,y,w,h) = calculate_boundbox(points[42:48]) #right eye\n",
    "    elif face_part == 5:\n",
    "        (x,y,w,h) = calculate_boundbox(points[29:36]) #nose\n",
    "    elif face_part == 6:\n",
    "        (x,y,w,h) = calculate_boundbox(points[48:68]) #mouth\n",
    "    return (x,y,w,h)\n",
    "\n",
    "while cv2.getWindowProperty('Video', 0) >= 0:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # detect faces in the grayscale frame\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    # loop over the face detections\n",
    "    for rect in rects:\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        for i in range(1,7):\n",
    "            (x,y,w,h) = get_face_boundbox(shape, i)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 1)\n",
    "\n",
    "        incl = calculate_inclination(shape[17], shape[26])\n",
    "\n",
    "        img = cv2.imread(\"nose.png\")\n",
    "        rows,cols = img.shape[0], img.shape[1]\n",
    "        M = cv2.getRotationMatrix2D((cols/2,rows/2),incl,1)\n",
    "        dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "        dst = rotate_bound(img, incl)\n",
    "        cv2.imshow('sprite',dst)\n",
    "\n",
    "        print(\"Pixels distance points in mouth: \", shape[66][1] - shape[62][1])\n",
    "\n",
    "        x,y, w, h = rect.left(), rect.top(), rect.width(), rect.height()\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "\n",
    "        # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        # and draw them on the image\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this filter puts heart emojis on your eyes when you open you mouth\n",
    "\n",
    "import dlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(\"http://192.168.0.102:8080/video\")\n",
    "img = cv2.imread(\"nose.png\")\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") \n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #convert the video into gray for faster processing\n",
    "    \n",
    "    faces = detector(gray) #detect faces in the gray frame \n",
    "    for face in faces:\n",
    "        \n",
    "        a1 = face.left()\n",
    "        b1 = face.top()\n",
    "        a2 = face.right()\n",
    "        b2 = face.bottom()\n",
    "        faceHeight = b2 - b1\n",
    "        faceWidth = a2- a1\n",
    "        #print(faceHeight,faceWidth)\n",
    "        #cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 3)\n",
    "        \n",
    "        landmarks = predictor(gray, face)\n",
    "        \n",
    "        leftEyeLeft = (landmarks.part(36).x,landmarks.part(36).y)\n",
    "        leftEyeRight = (landmarks.part(39).x,landmarks.part(39).y)\n",
    "        rightEyeLeft = (landmarks.part(42).x,landmarks.part(42).y)\n",
    "        rightEyeRight = (landmarks.part(45).x,landmarks.part(45).y)\n",
    "        \n",
    "        heartWidth = leftEyeRight[0] - leftEyeLeft[0]\n",
    "        heartWidth = 3 * heartWidth\n",
    "        heartHeight = heartWidth\n",
    "        \n",
    "        heartImage = cv2.resize(img, (heartWidth,heartHeight))\n",
    "        heartImageGray = cv2.cvtColor(heartImage, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        leftEyeCenter = ((leftEyeLeft[0] + leftEyeRight[0])/2,(leftEyeLeft[1] + leftEyeRight[1])/2)\n",
    "        leftTopLeft = (int(leftEyeCenter[0] - heartWidth / 2),int(leftEyeCenter[1] - heartHeight / 2))\n",
    "        leftBottomRight = (int(leftEyeCenter[0] + heartWidth / 2),int(leftEyeCenter[1] + heartHeight / 2))\n",
    "        \n",
    "        rightEyeCenter = ((rightEyeLeft[0] + rightEyeRight[0])/2,(rightEyeLeft[1] + rightEyeRight[1])/2)\n",
    "        rightTopLeft = (int(rightEyeCenter[0] - heartWidth / 2),int(rightEyeCenter[1] - heartHeight / 2))\n",
    "        rightBottomRight = (int(rightEyeCenter[0] + heartWidth / 2),int(rightEyeCenter[1] + heartHeight / 2))        \n",
    "        \n",
    "        x1 = landmarks.part(62).x\n",
    "        y1 = landmarks.part(62).y\n",
    "        x2 = landmarks.part(66).x\n",
    "        y2 = landmarks.part(66).y\n",
    "        \n",
    "        if (y2-y1)/faceHeight > 0.05:\n",
    "            print(\"mouth open\")\n",
    "            heartAreaLeft = frame[leftTopLeft[1] : leftTopLeft[1] + heartHeight,\n",
    "                              leftTopLeft[0] : leftTopLeft[0] + heartWidth]  \n",
    "            _, heartMask = cv2.threshold(heartImageGray, 25, 255, cv2.THRESH_BINARY_INV)\n",
    "            heartAreaNoHeartLeft = cv2.bitwise_and(heartAreaLeft,heartAreaLeft, mask = heartMask)\n",
    "            finalHeartLeft = cv2.add(heartAreaNoHeartLeft,heartImage)\n",
    "            frame[leftTopLeft[1] : leftTopLeft[1] + heartHeight,\n",
    "                         leftTopLeft[0] : leftTopLeft[0] + heartWidth]  = finalHeartLeft\n",
    "                  \n",
    "            heartAreaRight = frame[rightTopLeft[1] : rightTopLeft[1] + heartHeight,\n",
    "                              rightTopLeft[0] : rightTopLeft[0] + heartWidth]  \n",
    "            heartAreaNoHeartRight = cv2.bitwise_and(heartAreaRight,heartAreaRight, mask = heartMask)\n",
    "            finalHeartRight = cv2.add(heartAreaNoHeartRight,heartImage)\n",
    "            frame[rightTopLeft[1] : rightTopLeft[1] + heartHeight,\n",
    "                         rightTopLeft[0] : rightTopLeft[0] + heartWidth]  = finalHeartRight\n",
    "                  \n",
    "        \n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n",
      "998\n",
      "463\n",
      "998\n",
      "463\n",
      "998\n",
      "463\n",
      "998\n",
      "463\n",
      "998\n",
      "584\n",
      "1029\n",
      "584\n",
      "1029\n",
      "584\n",
      "1029\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f50b7e68b4eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#convert the video into gray for faster processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#detect faces in the gray frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(\"http://192.168.0.102:8080/video\")\n",
    "img = cv2.imread(\"heart.png\")\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") \n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #convert the video into gray for faster processing\n",
    "    \n",
    "    faces = detector(gray) #detect faces in the gray frame \n",
    "    for face in faces:\n",
    "        \n",
    "        a1 = face.left()\n",
    "        b1 = face.top()\n",
    "        a2 = face.right()\n",
    "        b2 = face.bottom()\n",
    "        faceHeight = b2 - b1\n",
    "        faceWidth = a2- a1\n",
    "        print(a1)\n",
    "        print(a2)\n",
    "        faceHeight = b2 - b1\n",
    "        faceWidth = a2- a1\n",
    "        #print(faceHeight,faceWidth)\n",
    "        #cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 3)\n",
    "        \n",
    "        landmarks = predictor(gray, face)\n",
    "        \n",
    "        leftEyeLeft = (landmarks.part(36).x,landmarks.part(36).y)\n",
    "        leftEyeRight = (landmarks.part(39).x,landmarks.part(39).y)\n",
    "        rightEyeLeft = (landmarks.part(42).x,landmarks.part(42).y)\n",
    "        rightEyeRight = (landmarks.part(45).x,landmarks.part(45).y)\n",
    "        \n",
    "        heartWidth = leftEyeRight[0] - leftEyeLeft[0]\n",
    "        heartWidth = 3 * heartWidth\n",
    "        heartHeight = heartWidth\n",
    "leftEyeLeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(679, 549)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftEyeLeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftEyeRight[0] - leftEyeLeft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
